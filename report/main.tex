\documentclass[11pt]{article}

% ---------- Packages ----------
\usepackage[a4paper,margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{siunitx}
\usepackage{amsmath,amssymb}
\usepackage{hyperref}
\usepackage[nameinlink,noabbrev]{cleveref}
\usepackage{xcolor}

\hypersetup{
  colorlinks=true,
  linkcolor=blue,
  citecolor=blue,
  urlcolor=blue
}

% ---------- Image root (adjust if needed) ----------
\newcommand{\imageroot}{artifacts/grids_with_ref}

% ---------- Title ----------
\title{DreamBooth: Personalizing Text-to-Image Diffusion Models for Subject-Driven Generation}
\author{}
\date{}

\begin{document}
\maketitle

\section{Introduction}
Large text-to-image diffusion models can synthesize high-quality, diverse images conditioned on natural-language prompts. However, they typically \emph{do not} reproduce a specific subject's identity reliably from only a few reference images. DreamBooth addresses this by fine-tuning a pre-trained text-to-image diffusion model using a handful of images (typically 3--5) of a target subject and binding a rare-token identifier to that instance. The resulting personalized model can render the same subject in new scenes, poses, and styles while preserving distinctive visual features.

\section{Background on Text-to-Image Diffusion Models}
Diffusion models learn a data distribution by reversing a fixed-length forward noising process. Let $x$ denote a clean image, $c$ a text-conditioning vector (obtained from a prompt via a text encoder), and let
\[
  z_t \coloneqq \alpha_t x + \sigma_t \,\epsilon,
  \quad \epsilon \sim \mathcal{N}(0, I),
\]
where $(\alpha_t, \sigma_t)$ follow a prescribed noise schedule and $t$ indexes diffusion time. A text-conditional denoiser $\hat{x}_{\theta}$ is trained to predict $x$ from $(z_t, c)$ via a weighted mean-squared error (MSE):
\begin{equation}
  \mathbb{E}_{x,\,c,\,\epsilon,\,t}\!\left[
    w_t\,\left\lVert \hat{x}_{\theta}\!\big(\alpha_t x + \sigma_t \epsilon,\; c\big) - x \right\rVert_2^2
  \right].
  \label{eq:base-mse}
\end{equation}
At inference, starting from Gaussian noise, the model iteratively denoises to produce a sample consistent with the prompt $c$. High-resolution outputs are obtained either through cascaded diffusion (base model + super-resolution modules) or via latent diffusion with a learned encoder/decoder.

\section{DreamBooth Methodology}
\paragraph{Prompting and rare-token identifiers.}
Each training image is paired with a prompt of the form ``a [unique-id] [class noun]'', e.g., ``a [V] dog''. The class noun anchors the model's class prior; the rare token binds to the specific instance.

\paragraph{Class-specific Prior Preservation Loss (PPL).}
Fine-tuning only on the few instance images can cause \emph{language drift} (the model forgets the broader class) and reduced diversity. DreamBooth augments training with an autogenous prior-preservation term: generate class-consistent samples $x_{\mathrm{pr}}$ using the \emph{frozen} base model with a generic class prompt (e.g., ``a dog''), and include them in the loss. The combined objective is
\begin{equation}
\begin{aligned}
  \mathbb{E}\big[ \;
    & w_t\,\big\lVert \hat{x}_{\theta}\!\big(\alpha_t x + \sigma_t \epsilon,\; c\big) - x \big\rVert_2^2
    \\
    &\quad +\;\lambda\, w_{t'}\,\big\lVert \hat{x}_{\theta}\!\big(\alpha_{t'} x_{\mathrm{pr}} + \sigma_{t'} \epsilon',\; c_{\mathrm{pr}}\big) - x_{\mathrm{pr}} \big\rVert_2^2
  \; \big],
\end{aligned}
\label{eq:ppl}
\end{equation}
where $(t,\epsilon,c)$ index the instance data and $(t',\epsilon',c_{\mathrm{pr}})$ index the prior-preservation samples. The weight $\lambda > 0$ controls the regularization strength.

\paragraph{Super-resolution fine-tuning.}
For cascaded pipelines, fine-tuning super-resolution (SR) modules with reduced noise augmentation helps preserve fine-grained instance details and avoids blurring/hallucination artifacts on the personalized subject.

\section{Experiments and Results}
We compare three training regimes---\textbf{Overfit}, \textbf{Underfit}, and \textbf{Balanced}---on a Stable Diffusion reproduction, averaging metrics across multiple prompts and seeds.

% \footnote{The accompanying CSV
% \texttt{extended\_comparison\_metrics.csv} contains per-prompt/per-seed results. The following table reports averages with standard deviations for robustness.}

\emph{Subject fidelity} is reported as identity similarity versus reference images (max and mean across references). \emph{Prompt adherence} is measured by a text--image similarity score. \emph{Diversity} is the mean pairwise cosine similarity among generations with the same prompt; lower values indicate greater visual variety. We additionally report the variability (standard deviation) of subject fidelity and text--image similarity to highlight stability across prompts.

\begin{table}[h]
  \centering
  \caption{Detailed quantitative comparison across DreamBooth training regimes (mean $\pm$ std). Bold values indicate the strongest result in each column.}
  \label{tab:regimes_detailed}
  \begin{tabular}{lccccc}
    \toprule
    Variant & Subj Max $\uparrow$ & Subj Mean $\uparrow$ & Text--Img $\uparrow$ & Diversity $\downarrow$ & Stability (Subj/Text) \\
    \midrule
    Balanced & 0.855 $\pm$ 0.066 & 0.843 & 0.288 $\pm$ 0.029 & 0.903 & 0.066 / 0.029 \\
    Overfit  & \textbf{0.881} $\pm$ 0.055 & \textbf{0.870} & 0.275 $\pm$ 0.031 & 0.915 & 0.055 / 0.031 \\
    Underfit & 0.784 $\pm$ 0.066 & 0.773 & \textbf{0.296} $\pm$ 0.029 & \textbf{0.880} & 0.066 / 0.029 \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{Discussion of regimes}
The results in \Cref{tab:regimes_detailed} confirm the trade-offs between fidelity, prompt adherence, and diversity:

\begin{itemize}
  \item \textbf{Overfit:} Highest subject fidelity (\textbf{0.881 max}, \textbf{0.870 mean}) but weakest prompt adherence (0.275) and lowest diversity (0.915). Qualitative results in \Cref{fig:portrait} show this regime reproduces the subject almost exactly but fails to generalize across contexts.
  \item \textbf{Underfit:} Best prompt adherence (\textbf{0.296}) and highest diversity (\textbf{0.880}) but lowest fidelity (0.784 / 0.773). In \Cref{fig:eiffel}, underfit outputs integrate better with scene prompts but often drift away from the subjectâ€™s identity.
  \item \textbf{Balanced:} A strong middle ground: fidelity (0.855 / 0.843) close to overfit, text--image score (0.288) close to underfit, and moderate diversity (0.903). Figures \Cref{fig:watercolor} and \Cref{fig:eiffel} illustrate that balanced models preserve subject features while still adapting to new styles and contexts.
\end{itemize}

In terms of stability, the reported standard deviations are small across all regimes ($\leq 0.066$), suggesting consistency across prompts. Balanced is slightly less stable in subject fidelity than overfit but achieves better trade-offs overall.

% \section{Experiments and Results}
% We compare three training regimes---\textbf{Overfit}, \textbf{Underfit}, and \textbf{Balanced}---on a Stable Diffusion reproduction, averaging metrics across multiple prompts and seeds.\footnote{The accompanying CSV
% \texttt{extended\_comparison\_metrics.csv} contains per-prompt/per-seed results from which the table below was aggregated.}
% \emph{Subject fidelity} is reported as identity similarity versus reference images (max and mean across references). \emph{Prompt adherence} is a text--image similarity score. \emph{Diversity} is the mean pairwise cosine similarity among generations with the same prompt; lower values indicate greater visual variety.

% \begin{table}[h]
%   \centering
%   \caption{Quantitative comparison across DreamBooth training regimes. Arrows indicate preferred direction. Lower Diversity implies higher visual variety.}
%   \label{tab:regimes}
%   \begin{tabular}{lcccc}
%     \toprule
%     Variant & Subj Max $\uparrow$ & Subj Mean $\uparrow$ & Text--Img $\uparrow$ & Diversity $\downarrow$ \\
%     \midrule
%     Balanced & 0.855 & 0.843 & 0.288 & 0.903 \\
%     Overfit  & 0.881 & 0.870 & 0.275 & 0.915 \\
%     Underfit & 0.784 & 0.773 & 0.296 & 0.880 \\
%     \bottomrule
%   \end{tabular}
% \end{table}

\subsection{Qualitative comparisons}
\Cref{fig:portrait} till \Cref{fig:running_in_a_park} show representative grids for identity preservation, style transfer, and recontextualization. Each grid (left to right) displays: target reference, base diffusion, overfit, underfit, and balanced outputs.

\begin{figure}[t]
  \centering
  \includegraphics[width=0.95\linewidth]{\imageroot/sks_dog_sitting_on_a_couch.png}
  \caption{Portrait photo of the subject across training regimes. Overfit maximizes identity but sacrifices diversity and controllability; Underfit improves diversity and prompt adherence but weakens identity; Balanced offers the best overall trade-off. Figure shows from left to right: target, base model, overfit finetuning, under fit, balanced.}
  \label{fig:portrait}
\end{figure}

\begin{figure}[t]
  \centering
  \includegraphics[width=0.95\linewidth]{\imageroot/a_watercolor_painting_of_sks_dog.png}
  \caption{Artistic rendering (watercolor). Balanced outputs preserve distinctive features while adapting to style constraints.}
  \label{fig:watercolor}
\end{figure}

\begin{figure}[t]
  \centering
  \includegraphics[width=0.95\linewidth]{\imageroot/sks_dog_in_front_of_the_eiffel_tower.png}
  \caption{Recontextualization: the subject in front of the Eiffel Tower. Balanced maintains both prompt adherence and identity.}
  \label{fig:eiffel}
\end{figure}

% \subsection{Discussion of regimes}
% \textbf{Overfit} (weak/absent prior preservation) yields very high subject fidelity but reduced prompt controllability and limited variety. \textbf{Underfit} (strong prior preservation) improves text adherence and diversity yet under-represents instance-specific features. \textbf{Balanced} appropriately tunes the prior loss and text-encoder updates to retain identity while remaining controllable across prompts.

\section{Applications}
DreamBooth supports recontextualization, style transfer, novel view synthesis, property edits, expression manipulation, and accessorization. Persistent identity enables, e.g., comics or creative assets featuring the same character across frames while preserving distinguishing attributes.

\section{Limitations and Societal Impact}
Observed failure modes include rare-context prompts, context--appearance entanglement, and overfitting to training settings when prompts are too close to the reference context. As with all realistic generative imagery, responsible use is essential given the potential for misuse.

\appendix
\section{Additional Grids}
All follow the same left-to-right layout (target reference, base diffusion, overfit, underfit, balanced).

% \begin{figure}[h]
%   \centering
%   \includegraphics[width=0.95\linewidth]{\imageroot/sks_dog_sitting_on_a_couch.png}
%   \caption{Subject sitting on a couch.}
% \end{figure}

\begin{figure}[h]
  \centering
  \includegraphics[width=0.95\linewidth]{\imageroot/sks_dog_wearing_sunglasses_street_photo.png}
  \caption{Street photo with sunglasses.}
\end{figure}

\begin{figure}[h]
  \centering
  \includegraphics[width=0.95\linewidth]{\imageroot/sks_dog_with_a_red_bandana__seed0.png}
  \caption{Accessorization: red bandana.}
\end{figure}

\begin{figure}[h]
  \centering
  \includegraphics[width=0.95\linewidth]{\imageroot/sks_dog_on_the_beach_at_sunset__seed0.png}
  \caption{Beach at sunset.}
\end{figure}

\begin{figure}[h]
  \centering
  \includegraphics[width=0.95\linewidth]{\imageroot/sks_dog_running_in_a_park__seed0.png}
  \caption{Running in a park.}
  \label{fig:running_in_a_park}
\end{figure}

\end{document}